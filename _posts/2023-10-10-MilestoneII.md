---
title: "Building a Machine Learning Model to Predict Party Affiliation"
date: 2023-10-10
published: true
tags: [ machine learning, riley, schenck]
excerpt: "Co-authored research paper with peers in my Master's of Applied Data Science" 

toc: true
toc_sticky: true
---

Excerpt below from a research paper I co-authored with peers in my Master's of Applied Data Science at the University of Michigan. The full paper can be found [here](https://docs.google.com/document/d/1RBbPwoc3AiKqKoiD9gY6l7JDxYlD2jDPxfXCyBe64Ig/edit?usp=sharing). 
All data used in this project came from the political consulting firm Aristotle. 

## Using Supervised Learning to Predict Party Preference
Knowing an individual’s party preference is valuable information for a political data consulting firm. A Republican candidate’s primary election campaign wouldn’t want to waste money serving ads to Democrats, and a Democratic candidate's general election campaign focused on getting the vote out doesn’t want their volunteers knocking on Republicans’ doors. Aristotle has already constructed numerous features in the dataset that classify individuals into various political profiles such as someone likely to watch liberal news channels or support border security measures, or as a likely Democrat or likely Republican based on public voting data. Even with such measures, approximately  31% of individuals in Aristotle’s database lack party registration or inferred party preference. A feature that represents a machine learning model’s prediction of party preference for all rows could be a valuable addition to their dataset.
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone1-barchart1.png)

We decided that our target variable for the party preference modeling tasks (INFER_PARTY) would include only the data that should be most predictive of whether an individual would prefer a Democratic or Republican candidate: actual voter data indicating whether individuals voted Democrat or Republican in elections from 2016 to 2022, with the most recent election we have data for taking precedent. Our  target variable is observed in  112,283 rows from the original 500,000 row dataset. The resulting dataset containing our target variable is fairly balanced between the different states, representing a wide sample of American voters (see gray areas of Figure 1), with the exception of California which doesn’t provide election data on voters.

We pursued two strategies for modeling party preference: 1) exclude all Aristotle features built with voter data (used to create our target feature) , relying on less predictive demographic, census, donation, and consumer data to avoid any data leakage issues, and 2) include the more predictive Aristotle-engineered features that may contain data from our target variable with the aim of finding an optimal feature weighting system for predicting party preference, and use the survey data set as a validation set to see if the data leakage affects the models’ performance on unseen, real world data. .

First, we identified 17 features that could include data from our target feature INFER_PARTY and removed them, resulting in a dataset with shape 112,283, 188. Next, we set up our preprocessing pipeline, including one-hot encoding for categorical variables and mean imputation for missing values from numerical columns, and implemented scaling using StandardScaler for use in Logistic Regression and Support Vector Machine (SVM) models. In addition to Logistic Regression and SVM, we also experimented with three different tree-based models: XGBoost, GBM, and Random Forest. Our models achieved accuracy scores of 77%, 76%, 75%, 76%, and 51% respectively. See the top 15 features that the XGBoost model used to split on within its decision trees in Figure 5.
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone2-topfeatures.png)

We then used the trained models to predict the party for each row an unseen 1,560 row survey dataset (none of the rows with survey data were included in the models’ training or test sets) to see how our models performed at predicting real world data collected in September on voters’ preferences for President in 2024. We coded as Democrats survey respondents who indicated support for the two Democratic candidates (President Biden and Marianne Williamson), and coded as Republican respondents who indicated support for any of the candidates running in the Republican primary. Respondents who indicated support for Robert Kennedy Jr. (Independent) and Undecided were dropped from the dataset. Our best model, XGBoost, achieved 67% accuracy at predicting the actual party preferences of the 2024 presidential candidate survey respondents.  

We next attempted the second proposed approach to model party preference: include the more predictive Aristotle-engineered features for modeling that may introduce target data leakage. For example, the “PRFL_LIBERAL_NEWS” feature, according to Aristotle’s records, is coded as ‘Yes’ if an individual has a liberal or moderate political ideology code, a registered Democrat code, or has voted in a Democratic primary (which would include data from our target variable), or is a Clinton, Biden, Obama or Sanders supporter. On the survey dataset, 73% of individuals who are coded as “Yes” for PRFL_LIBERAL_NEWS indicated they support a Democratic candidate for president in the survey demonstrating that it is a highly predictive feature.

After including these 17 features back in our dataset, we used the same five machine learning algorithms to train our models. For this second modeling strategy the only evaluation metric that mattered was the models’ accuracy at predicting the candidate preference responses in the validation survey dataset since the test set accuracy metric could be artificially inflated by data leakage between our target feature and predictor features. 
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone3.5-modeleval.png)

The heatmap below shows F1, Precision, and Recall scores by target variable and model on the validation survey dataset. Looking at the “Accuracy” row, you can see that our most accurate model at predicting party preference on the validation survey dataset was XGBoost at 74.68%. Overall the model does much better at predicting Republicans with an F1 score of 79% vs 68% for Democrats. The precision score of 62.26% for Democrats shows that the model is showing a bias at over predicting the Democrat class with relatively more false positives, which is also reflected in the failure analysis carried out below. The other two tree-based models and the Logistic Regression model all achieved accuracy scores within a percentage point of the XGBoost model, whereas the SVM model likely suffered from the target data leakage issue with a survey validation dataset accuracy score of 38% despite achieving a test set score of 95%. 
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone4-modeleval.png)

The bar chart below shows the top 10 features of each of the three decision tree-based models. All 3 models’ top 10 features are one-hot encoded categorical values, and 10 of those 15 different features are from engineered Aristotle features (those with the “PRFL_” prefix). Other features include a feature indicating whether an individual has donated to a Democratic campaign and demographic and census data features identifying Christian families, hispanic individuals, and whether an individual resides in an urban or rural county. 
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone5-modeleval.png)

All of the tree-based models improve upon the Aristotle-engineered features PRFL_LIBERAL_NEWS and PRFL_IMMIGRATION_REFORM’s baselines of 66% and 67%  accuracy on the survey validation set by 6-9%.  The differences in their “Feature Importance” scores in Figure 6 are more indicative of the different splitting strategies of the tree-based algorithms than relative feature importance in their models. XGBoost’s very high feature importance of “Liberal News'' doesn’t mean that the model is finding it far more predictive than the rest of the features, rather that it is frequently splitting on that feature depending on whether it’s encoded as “Yes” or “NaN, creating two distinct paths before splitting on another feature and continuing this process of splitting down different paths to reach a prediction until the algorithm reaches its “max depth” hyperparameter (in our case the default of 7 splits). For example, the XGBoost model splits on PRFL_LIBERAL_NEWS == “Yes” 52% of the time. However, when we remove LIBERAL_NEWS as a feature, the XGBoost model simply replaces it with PRFL_IMMIGRATION_REFORM, splitting on it 56% of the time with accuracy on the test and survey validation sets decreasing just .05% and 1.14% respectively.

In contrast to the tree-based and logistic regression models, the target data leakage issue clearly corrupted the SVM model. While the SVM achieves similar performance to the other models on the test set at 94% accuracy, it performs even worse on the validation survey data set than a coin flip. While tree based models inherently capture non-linear relationships and interactions between features, SVM works by finding linear decision boundaries for classification among all the individual data points. The SVM likely draws a decision boundary utilizing this leakage that then fails to generalize to unseen data.

It appears that the decision trees allow us to leverage data interactions and a manageable amount of target data leakage from our target variable to capture underlying patterns that can then generalize well on unseen data, such as the validation survey data set.

## Failure Analysis
Overall, our final XGBoost model demonstrates bias in the survey data set towards predicting for Democrats, where compared with the actual party preferences of survey responses there are 22% more predicted Democrat responses than actual Democrat responses, and 12% fewer predicted Republican responses than actual Republican responses.  

The 75% accuracy of the XGBoost model on the survey data validation set can be further analyzed by breaking the accuracy score into two: 547 survey respondents that have the 2016-22 voter data used for our target variable, and 986 survey respondents that don’t. Our model accurately predicts the former’s party preference at 83%, and the latter’s at 70%.

The failure analysis shows that 16.4% of the survey respondents who have voter data from our target variable present indicated support for a presidential candidate whose party is the opposite of the last party the individual supported on record. That discrepancy explains most of the 18% error rate when predicting for this group. The crosstab on the right shows that in 79 instances (14%), the model correctly predicted the party corresponding to an individual’s voting history, yet the individual indicated in the survey that they planned on voting for a candidate of the opposite party. The model almost always decided to classify those in this group as likely to vote for the same party as their voter data indicates which is unsurprising considering the model showed 96% accuracy at predicting the target party when voter data is available. 
![Plot1]({{ site.url }}{{ site.baseurl }}/assets/images/milestone6-failureanalysis.png)

While “Liberal News” does a very good job of predicting party preference when voter data is available, it struggles as a predictor for the 986 voters that don’t have  2016-22 voter data available. It’s likely not the only feature that contributes to the overclassification of Democratic voters, but it appears to be a big contributor, where voters with Republican candidate preferences are misclassified as Democrats 42% of the time when PRFL_LIBERAL_NEWS is encoded as “Yes.” At the same time, removing PRFL_LIBERAL_NEWS as a feature in model training only weakens the model’s performance both on the test set and on generalizing on the survey data. Perhaps some peoples’ politics has shifted since the feature was initially created, and the majority of that shift has been from Democrat to Republican, contributing to the overclassification of Democrats. Regardless of the reason, a likely profitable area of future work for Aristotle to pursue would involve evaluating and adjusting previously engineered features such as “Liberal News,” which would help to further improve the performance of the new machine learning party preference feature created by our XGBoost model.

## Discussion
We learned with our supervised party preference modeling task that we could accurately predict the party of the preferred presidential candidate of survey respondents 75% of the time with our XGBoost model trained on 217 features, a significant improvement of 9% over the accuracy of the most predictive Aristotle-engineered feature. While the model performed significantly worse predicting party preference for the 224 individuals from the survey data set coded by Aristotle as “Non-Partisans” and “Unknowns” for party preference, an accuracy of 59% is still significantly better than a coin flip and provides Aristotle with a good starting point for attempting to predict party preference among that particularly challenging class of voters. A surprise finding was that 18% of our respondents with voter data from 2016-2022 indicated support for the opposite party of their most recent voter data on file. This suggests that there may be a significant error rate in the survey responses pulling our accuracy scores downwards since it seems unlikely that nearly a fifth of voters have switched their party allegiance in the last few years.   

Another surprising finding was how similar the supervised party preference models performed on both the test set and the survey validation set (with the exception of the SVM model, which likely was corrupted by data leakage from the target variable). We were hopeful that the tree-based models would overcome the data leakage issue since the target variable leakage isn’t an exact match with any of the predictor variables, but is used in their construction, allowing the decision tree-based models to effectively weight and sort through the different features to find predictive combinations of features. It was a complete surprise however that the logistic regression model was just one percent less accurate than the best tree-based model on the survey validation dataset, especially when considering there were 605 total predictor features after one-hot encoding was applied in preprocessing. The fact that four different machine learning algorithms correctly predicted survey respondents’ party preference between 73-75% shows that there is a very strong signal for party preference within the data. 


### Full Report

The full report can be found [here](https://docs.google.com/document/d/1RBbPwoc3AiKqKoiD9gY6l7JDxYlD2jDPxfXCyBe64Ig/edit?usp=sharing). 
